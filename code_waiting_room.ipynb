{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code waiting-room\n",
    "Code that I am unsure whether we are gonna be using, so for now it is just situated right here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training implementation *(from week 5)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "num_steps = 5 # 5_000\n",
    "step = 0\n",
    "epoch = 0\n",
    "\n",
    "for epoch in range(len(EPOCHS)):\n",
    "    for batch in train_loader:\n",
    "        # concatenate the `token_ids``\n",
    "        batch_token_ids = make_batch(batch)\n",
    "        batch_token_ids = batch_token_ids.to(DEVICE)\n",
    "\n",
    "        # forward through the model\n",
    "        optimizer.zero_grad()\n",
    "        batch_logits = rnn2(batch_token_ids)\n",
    "\n",
    "        # compute the loss (negative log-likelihood)\n",
    "        p_ws = torch.distributions.Categorical(logits=batch_logits) \n",
    "\n",
    "        # Exercise: write the loss of the RNN language model\n",
    "        # hint: check the doc https://pytorch.org/docs/stable/distributions.html#categorical\n",
    "        # NB: even with the right loss, training is slow and the generated samples won't be very good.\n",
    "        #\n",
    "        # NOTE:\n",
    "        # We need to find the negative log-likelihood, which we do by utilising the logarithmic_probabilities, function\n",
    "        # of a Categorical object. By summing we take the logarithmic probabilities down to one-dimension, then we \n",
    "        # elect to scale down to a scalar by finding the mean of this one-dimensional vector:\n",
    "        loss = -torch.sum(p_ws.log_prob(batch_token_ids), dim=1).mean()\n",
    "\n",
    "        # backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Report\n",
    "        if step % 5 ==0 :\n",
    "            loss = loss.detach().cpu()\n",
    "            pbar.set_description(f\"epoch={epoch}, step={step}, loss={loss:.1f}\")\n",
    "\n",
    "        # save checkpoint\n",
    "        if step % 50 ==0 :\n",
    "            torch.save(rnn.state_dict(), checkpoint_file)\n",
    "        if step >= num_steps:\n",
    "            break\n",
    "    epoch += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d96a798051220adb8d47ede7819712d4980d7e1ecee887457e300fc8d0177c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
