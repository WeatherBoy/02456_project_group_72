{"cells":[{"cell_type":"markdown","metadata":{},"source":["# A notebook on the Eron Dataset from Kaggle\n","This notebook was found through the following [link](https://www.kaggle.com/code/conniedeng/nlp-eron-dataset?fbclid=IwAR3k6TfBRz842eBrj3l3pOY9a3qSiO3r1JqhI2UeLCx9slJU4RrQvrt-D0w)."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-12-06T06:24:01.78116Z","iopub.status.busy":"2021-12-06T06:24:01.780624Z","iopub.status.idle":"2021-12-06T06:24:01.794426Z","shell.execute_reply":"2021-12-06T06:24:01.793793Z","shell.execute_reply.started":"2021-12-06T06:24:01.781059Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["# Extracting email's body and subject from Eron dataset into Pandas dataframe"]},{"cell_type":"markdown","metadata":{},"source":["* new dataset\n","* seperate headers\n","* https://www.kaggle.com/nagasai524/spam-email-classification-using-word2vec"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:01.796405Z","iopub.status.busy":"2021-12-06T06:24:01.795789Z","iopub.status.idle":"2021-12-06T06:24:01.985087Z","shell.execute_reply":"2021-12-06T06:24:01.983893Z","shell.execute_reply.started":"2021-12-06T06:24:01.796371Z"},"trusted":true},"outputs":[],"source":["# Input data files are available in the \"../input/\" directory.\n","filepath = \"/kaggle/input/spam-mails-dataset/spam_ham_dataset.csv\"\n","\n","# Read the data into a pandas dataframe called emails\n","emails=pd.read_csv(\"/kaggle/input/spam-mails-dataset/spam_ham_dataset.csv\")\n","\n","print(\"Successfully loaded {} rows and {} columns!\".format(emails.shape[0], emails.shape[1]))\n","emails.head()"]},{"cell_type":"markdown","metadata":{},"source":["I have noticed that these emails are all lowercase; I've looked at some other data sets as well (least the ones that have spam/ham labeles and they also seem to be lowercase)\n","\n","I honestly think having caps would be amazing because I'm sure spam emails include a ton more caps - but uh yea"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:01.987284Z","iopub.status.busy":"2021-12-06T06:24:01.986952Z","iopub.status.idle":"2021-12-06T06:24:01.994632Z","shell.execute_reply":"2021-12-06T06:24:01.993448Z","shell.execute_reply.started":"2021-12-06T06:24:01.987239Z"},"trusted":true},"outputs":[],"source":["def get_email_subject(email):\n","    subject = email[0:email.find('\\r\\n')]\n","    subject = subject. replace('Subject: ', '')\n","    return subject\n","\n","def get_email_body(email):\n","    body = email[email.find('\\r\\n')+2:]\n","    return body"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:01.997456Z","iopub.status.busy":"2021-12-06T06:24:01.99696Z","iopub.status.idle":"2021-12-06T06:24:02.052578Z","shell.execute_reply":"2021-12-06T06:24:02.051547Z","shell.execute_reply.started":"2021-12-06T06:24:01.997413Z"},"trusted":true},"outputs":[],"source":["# cleaning of columns\n","email_df = emails.drop(['Unnamed: 0', \"label_num\"], axis = 1)\n","\n","# get the subject and body of email\n","email_df[\"subject\"] = email_df[\"text\"].apply(lambda x: get_email_subject(x))\n","email_df[\"body\"] = email_df[\"text\"].apply(lambda x: get_email_body(x))\n","\n","# ridding of the text column (unless we need it)\n","email_df = email_df.drop([\"text\"], axis = 1)\n","\n","email_df\n","\n","# expand default pandas display options to make emails more clearly visible when printed\n","pd.set_option('display.max_colwidth', 200)\n","\n","# from here email_df is our dataframe\n","email_df.head() # you could do print(bodies_df.head()), but Jupyter displays this nicer for pandas DataFrames"]},{"cell_type":"markdown","metadata":{},"source":["# Text/Data Pre-processing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:02.055049Z","iopub.status.busy":"2021-12-06T06:24:02.054465Z","iopub.status.idle":"2021-12-06T06:24:02.059689Z","shell.execute_reply":"2021-12-06T06:24:02.058884Z","shell.execute_reply.started":"2021-12-06T06:24:02.054998Z"},"trusted":true},"outputs":[],"source":["# hyperparameters \n","maxtokens = 200 # the maximum number of tokens per document\n","maxtokenlen = 100 # the maximum length of each token"]},{"cell_type":"markdown","metadata":{},"source":["**Tokenization** (Maybe we will have multiple tokenization methods; you can put how you wana tokenize down here)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:02.062783Z","iopub.status.busy":"2021-12-06T06:24:02.062198Z","iopub.status.idle":"2021-12-06T06:24:02.073854Z","shell.execute_reply":"2021-12-06T06:24:02.072973Z","shell.execute_reply.started":"2021-12-06T06:24:02.062718Z"},"trusted":true},"outputs":[],"source":["# Tokenization method 1\n","# this is tokenization split by white sapce\n","def tokenize_1(row):\n","    if row is None or row is '':\n","        tokens = \"\"\n","    else:\n","        tokens = str(row).split(\" \")[:maxtokens]\n","    return tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:02.07861Z","iopub.status.busy":"2021-12-06T06:24:02.077701Z","iopub.status.idle":"2021-12-06T06:24:03.899468Z","shell.execute_reply":"2021-12-06T06:24:03.898533Z","shell.execute_reply.started":"2021-12-06T06:24:02.078058Z"},"trusted":true},"outputs":[],"source":["from nltk.tokenize import word_tokenize, wordpunct_tokenize"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:03.900976Z","iopub.status.busy":"2021-12-06T06:24:03.900747Z","iopub.status.idle":"2021-12-06T06:24:03.90533Z","shell.execute_reply":"2021-12-06T06:24:03.904439Z","shell.execute_reply.started":"2021-12-06T06:24:03.900948Z"},"trusted":true},"outputs":[],"source":["# Tokenization method 2\n","# split of white space AND punctuation $3.88 --> '3', '.', '88'\n","def tokenize_2(row):\n","    return wordpunct_tokenize(str(row))[:maxtokens]"]},{"cell_type":"markdown","metadata":{},"source":["**Regular Expression to remove  unnecessary characters** (removing \\n new lines, symbols?, this could also include links)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:03.906753Z","iopub.status.busy":"2021-12-06T06:24:03.906476Z","iopub.status.idle":"2021-12-06T06:24:03.918476Z","shell.execute_reply":"2021-12-06T06:24:03.91765Z","shell.execute_reply.started":"2021-12-06T06:24:03.906693Z"},"trusted":true},"outputs":[],"source":["import re\n","\n","# this covers lower() tokens\n","def reg_expressions(row):\n","    row = re.sub(r'[\\r\\n]', \"\", row)\n","    return row"]},{"cell_type":"markdown","metadata":{},"source":["**Stop-word removal** (removing unimportant words)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:03.921788Z","iopub.status.busy":"2021-12-06T06:24:03.920977Z","iopub.status.idle":"2021-12-06T06:24:03.939496Z","shell.execute_reply":"2021-12-06T06:24:03.938798Z","shell.execute_reply.started":"2021-12-06T06:24:03.921722Z"},"trusted":true},"outputs":[],"source":["import nltk\n","stopwords = nltk.corpus.stopwords.words('english')\n","print(stopwords[:10])\n","\n","def stop_word_removal(row):\n","    token = [token for token in row if token not in stopwords]\n","    return token"]},{"cell_type":"markdown","metadata":{},"source":["**Stemming** (removing endings of words, -ing, -ly...)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:03.941301Z","iopub.status.busy":"2021-12-06T06:24:03.940526Z","iopub.status.idle":"2021-12-06T06:24:03.945624Z","shell.execute_reply":"2021-12-06T06:24:03.944994Z","shell.execute_reply.started":"2021-12-06T06:24:03.941262Z"},"trusted":true},"outputs":[],"source":["def stemming(row):\n","    port_stemmer = nltk.stem.porter.PorterStemmer()\n","    token = [port_stemmer.stem(token) for token in row]\n","    return token"]},{"cell_type":"markdown","metadata":{},"source":["**Lemmatization** (convert into root word)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:03.947266Z","iopub.status.busy":"2021-12-06T06:24:03.946896Z","iopub.status.idle":"2021-12-06T06:24:03.960898Z","shell.execute_reply":"2021-12-06T06:24:03.959992Z","shell.execute_reply.started":"2021-12-06T06:24:03.947235Z"},"trusted":true},"outputs":[],"source":["def lemmatization(row):\n","    lem = nltk.stem.wordnet.WordNetLemmatizer()\n","    token = [lem.lemmatize(token) for token in row]\n","    return token"]},{"cell_type":"markdown","metadata":{},"source":["**Final utility in preprocessing data connecting all these preprocessing techniques**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:03.962646Z","iopub.status.busy":"2021-12-06T06:24:03.962261Z","iopub.status.idle":"2021-12-06T06:24:03.975075Z","shell.execute_reply":"2021-12-06T06:24:03.974002Z","shell.execute_reply.started":"2021-12-06T06:24:03.962607Z"},"trusted":true},"outputs":[],"source":["'''\n","Preprocess a string.\n",":parameter\n","    :param text: string - name of column containing text\n","    :param lst_stopwords: list - list of stopwords to remove\n","    :param flg_stemm: bool - whether stemming is to be applied\n","    :param flg_lemm: bool - whether lemmitisation is to be applied\n",":return\n","    cleaned text\n","'''\n","def utils_preprocess_text(text, flg_tokenize=1,flg_stemm=False, flg_lemm=True, flg_stopwords=True):\n","    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n","    text = reg_expressions(text)\n","\n","    ## Tokenize (convert from string to list)\n","    if flg_tokenize == 1:\n","        text = tokenize_1(text)\n","\n","    elif flg_tokenize == 2:\n","        text = tokenize_2(text)\n","    \n","    # remove Stopwords\n","    if flg_stopwords == True:\n","        text = stop_word_removal(text)\n","        \n","    ## Stemming (remove -ing, -ly, ...)\n","    if flg_stemm == True:\n","        text = stemming(text)\n","        \n","    ## Lemmatisation (convert the word into root word)\n","    if flg_lemm == True:\n","        text = lemmatization(text)\n","            \n","    ## back to string from list\n","    text = \" \".join(text)\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:03.976747Z","iopub.status.busy":"2021-12-06T06:24:03.976352Z","iopub.status.idle":"2021-12-06T06:24:18.41089Z","shell.execute_reply":"2021-12-06T06:24:18.410287Z","shell.execute_reply.started":"2021-12-06T06:24:03.976704Z"},"trusted":true},"outputs":[],"source":["email_df[\"text_clean\"] = email_df[\"body\"].apply(lambda x: utils_preprocess_text(x, flg_tokenize=2, flg_stemm=True, flg_lemm=True, flg_stopwords=True))\n","email_df"]},{"cell_type":"markdown","metadata":{},"source":["# Getting Training and Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:18.412256Z","iopub.status.busy":"2021-12-06T06:24:18.411932Z","iopub.status.idle":"2021-12-06T06:24:18.79614Z","shell.execute_reply":"2021-12-06T06:24:18.795236Z","shell.execute_reply.started":"2021-12-06T06:24:18.412227Z"},"trusted":true},"outputs":[],"source":["import seaborn as sns\n","sns.countplot(x=\"label\",data=email_df,order=['spam','ham'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:18.797533Z","iopub.status.busy":"2021-12-06T06:24:18.797295Z","iopub.status.idle":"2021-12-06T06:24:18.807707Z","shell.execute_reply":"2021-12-06T06:24:18.806883Z","shell.execute_reply.started":"2021-12-06T06:24:18.797503Z"},"trusted":true},"outputs":[],"source":["email_df[\"label\"].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["The ratio between spam and ham is **1499:3672** in the complete dataset. We will maintain this ratio between spam and ham for the training and test dataset.\n","\n","We will also split the dataset into a 80%:20% where the training set will be 80% and the test set will be 20%"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:18.809392Z","iopub.status.busy":"2021-12-06T06:24:18.809153Z","iopub.status.idle":"2021-12-06T06:24:18.837173Z","shell.execute_reply":"2021-12-06T06:24:18.836345Z","shell.execute_reply.started":"2021-12-06T06:24:18.809365Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# random_state 0 makes sure that the data split is consistently the same (so the random sampling does not keep changing)\n","train, test = train_test_split(email_df, test_size=0.20, stratify=email_df[\"label\"], random_state=0)"]},{"cell_type":"markdown","metadata":{},"source":["**Training data set**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:18.838718Z","iopub.status.busy":"2021-12-06T06:24:18.838503Z","iopub.status.idle":"2021-12-06T06:24:19.025497Z","shell.execute_reply":"2021-12-06T06:24:19.024673Z","shell.execute_reply.started":"2021-12-06T06:24:18.838692Z"},"trusted":true},"outputs":[],"source":["sns.countplot(x=\"label\",data=train, order=['spam','ham'])\n","print(train[\"label\"].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:19.02762Z","iopub.status.busy":"2021-12-06T06:24:19.02711Z","iopub.status.idle":"2021-12-06T06:24:19.22235Z","shell.execute_reply":"2021-12-06T06:24:19.221482Z","shell.execute_reply.started":"2021-12-06T06:24:19.027569Z"},"trusted":true},"outputs":[],"source":["sns.countplot(x=\"label\",data=test, order=['spam','ham'])\n","print(test[\"label\"].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:19.223808Z","iopub.status.busy":"2021-12-06T06:24:19.223548Z","iopub.status.idle":"2021-12-06T06:24:19.228141Z","shell.execute_reply":"2021-12-06T06:24:19.22727Z","shell.execute_reply.started":"2021-12-06T06:24:19.223777Z"},"trusted":true},"outputs":[],"source":["email_train_df = train\n","email_test_df = test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:19.229799Z","iopub.status.busy":"2021-12-06T06:24:19.229492Z","iopub.status.idle":"2021-12-06T06:24:19.250269Z","shell.execute_reply":"2021-12-06T06:24:19.249324Z","shell.execute_reply.started":"2021-12-06T06:24:19.229769Z"},"trusted":true},"outputs":[],"source":["email_train_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:19.251803Z","iopub.status.busy":"2021-12-06T06:24:19.251221Z","iopub.status.idle":"2021-12-06T06:24:19.255304Z","shell.execute_reply":"2021-12-06T06:24:19.254669Z","shell.execute_reply.started":"2021-12-06T06:24:19.251764Z"},"trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-06T06:24:19.257341Z","iopub.status.busy":"2021-12-06T06:24:19.256487Z","iopub.status.idle":"2021-12-06T06:24:21.875955Z","shell.execute_reply":"2021-12-06T06:24:21.875018Z","shell.execute_reply.started":"2021-12-06T06:24:19.257302Z"},"trusted":true},"outputs":[],"source":["countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english')\n","tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n","\n","count_wm = countvectorizer.fit_transform(email_train_df[\"text_clean\"])\n","tfidf_wm = tfidfvectorizer.fit_transform(email_train_df[\"text_clean\"])\n","\n","count_tokens = countvectorizer.get_feature_names()\n","tfidf_tokens = tfidfvectorizer.get_feature_names()\n","\n","\n","df_countvect = pd.DataFrame(data = count_wm.toarray(),columns = count_tokens)\n","df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),columns = tfidf_tokens)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
