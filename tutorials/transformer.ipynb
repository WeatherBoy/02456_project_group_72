{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt at supervised transformer\n",
    "Trying to implement a transformer as per the [(\"Attention is All You Need\", Wasrani et al. (2016))](https://arxiv.org/abs/1706.03762) paper and the **week 5 notebook**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All your imports are belong to us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant definition and other setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# define the device to use\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "num_steps = 5 # 5_000\n",
    "step = 0\n",
    "epoch = 0\n",
    "\n",
    "for epoch in range(len(EPOCHS)):\n",
    "    for batch in train_loader:\n",
    "        # concatenate the `token_ids``\n",
    "        batch_token_ids = make_batch(batch)\n",
    "        batch_token_ids = batch_token_ids.to(DEVICE)\n",
    "\n",
    "        # forward through the model\n",
    "        optimizer.zero_grad()\n",
    "        batch_logits = rnn2(batch_token_ids)\n",
    "\n",
    "        # compute the loss (negative log-likelihood)\n",
    "        p_ws = torch.distributions.Categorical(logits=batch_logits) \n",
    "\n",
    "        # Exercise: write the loss of the RNN language model\n",
    "        # hint: check the doc https://pytorch.org/docs/stable/distributions.html#categorical\n",
    "        # NB: even with the right loss, training is slow and the generated samples won't be very good.\n",
    "        #\n",
    "        # NOTE:\n",
    "        # We need to find the negative log-likelihood, which we do by utilising the logarithmic_probabilities, function\n",
    "        # of a Categorical object. By summing we take the logarithmic probabilities down to one-dimension, then we \n",
    "        # elect to scale down to a scalar by finding the mean of this one-dimensional vector:\n",
    "        loss = -torch.sum(p_ws.log_prob(batch_token_ids), dim=1).mean()\n",
    "\n",
    "        # backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Report\n",
    "        if step % 5 ==0 :\n",
    "            loss = loss.detach().cpu()\n",
    "            pbar.set_description(f\"epoch={epoch}, step={step}, loss={loss:.1f}\")\n",
    "\n",
    "        # save checkpoint\n",
    "        if step % 50 ==0 :\n",
    "            torch.save(rnn.state_dict(), checkpoint_file)\n",
    "        if step >= num_steps:\n",
    "            break\n",
    "    epoch += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d96a798051220adb8d47ede7819712d4980d7e1ecee887457e300fc8d0177c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
